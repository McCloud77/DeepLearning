{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Recurrent Neural Network to classify sentiment on IMDB data\n",
    "The IMDB data consists of 25000 training sequences and 25000 test sequences. The outcome is binary (positive/negative) and both outcomes are equally represented in both the training and the test set.\n",
    "\n",
    "Word embedding is a technique where words are encoded as real-valued vectors in a high-dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.  \n",
    "The embedding layer takes arguments that define the mapping including the maximum number of expected words, also called the vocabulary size (e.g. the largest integer value that will be seen as an integer). The layer also allows you to specify the dimensionality for each word vector, called the output dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 500  # maximum length of a sequence - truncate after this\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "## Load in the data.  The function automatically tokenizes the text into distinct integers\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features) # 5,000 most used words in the dataset\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 4998\n",
      "Review length: mean 234.76 words (172.911495)\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words:\", len(np.unique(np.hstack(X))))\n",
    "# Summarize review length\n",
    "result = [len(x) for x in X]\n",
    "print(\"Review length: \" + \"mean %.2f words (%f)\" % (np.mean(result), np.std(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 500)\n",
      "x_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "# This pads with zeros (or truncates) the sequences so that they are of the maximum length\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          1,   14,  201,  100,   55,   73,   30,    4,  118,    2,  126,\n",
       "          5,   15,    9,  660,    6,   87,  855, 1069,    4,    2,    2,\n",
       "          2,   52,    2,    8,  403,   43,  107,   10,   10,   51,   93,\n",
       "          2,   38, 1731,   60,    8,    4,  118,    7,    4,  118,    9,\n",
       "         15,   12,  122,   24,   43,    2,   87,  356,    2,    5, 1089,\n",
       "        212,   21,   12,   82,    2,    2,    2,    2,    2,    5,   33,\n",
       "        175, 1589,    4,   87, 1031,  653,   15,   16,    2,   11, 3513,\n",
       "         33,    4,   58,   10,   10,   13,  377,  149,   14,  123,   17,\n",
       "          6, 2343,    5,  112,   11, 4299,    7,  285,   44,   12,    4,\n",
       "       4164, 2399,   16, 1739,   11,  410,   14,   16,    4,   86,   58,\n",
       "         13,  219,  147,   84,  772,    2,    2,    5,    4,    2,    7,\n",
       "         32,    4,  105,   16,  230, 4026,   39,    4, 2968, 4039,    2,\n",
       "        131,  110,   11, 2397,  938,   33,    4,   58,   95,   50,   71,\n",
       "          4,    2, 2022,    2, 1880,  134,   84,   60,    4,    2, 3197,\n",
       "         19,   68, 4345, 1483,    8,  113,    5,    2,  486,   71, 3495,\n",
       "         36,   71, 2685,    7,    4,  162,    2,   15,   62,  216,    8,\n",
       "         28,   35, 1488,   23,  113,    5,  405,  469,    4,  182,   11,\n",
       "          4,  582, 2068, 3486,   10,   10,    7,  265,    4,  863,   16,\n",
       "        680,    5,  777,    4,  326,   15,  294,   37,   16,  581,    4,\n",
       "          2,  925,  100,   30,   38,    2,    5,    2,   16, 4198,  572,\n",
       "          8, 2397, 1531,   18,  937, 2652,   16,  210, 4786,   17,  278,\n",
       "          5,  183,  252,    4,  904,   16, 2399,   11,  298,  102,    5,\n",
       "        248,   21,   24,   19,    4, 3033,    7,    2,   42,    4,   20,\n",
       "          2,    5,   48,    4,  863,   16,   24, 1767,  195,    4,  817,\n",
       "         34,   63,   12,   16, 4977,  562,   12,    8,    4, 1572,    6,\n",
       "        986, 1586,   15,   16,   24,   66,   44,  383,   42,   60,  883,\n",
       "          2,    2,  111,   21,   12,  215,   28,  317,   49,   84,    2,\n",
       "         68,    2,   11, 2793,   33,    4,   58,  553,   10,   10,    2,\n",
       "        679,   46,    8,   30,   11,   38,  111,  771,    2,   12,    2,\n",
       "       1404,    7,   94,   58, 1208,  682,    2,   32,    2, 4164,    4,\n",
       "       4147,    7,  921,    2,    5,    2,    2,    5,  915,    2, 1000,\n",
       "        950,   34,    4,    2,  336,  492, 1519,   12,    9,   99,   78,\n",
       "         14,  201,   47,   24,   77,    2,   11,    6,  194,   96,    5,\n",
       "         32,  148,  574,  348, 1109,   18, 1855,    6, 3191,    2,    7,\n",
       "          6,  813,   58,    5,  273,    5,    2,   32,    4,    2,    5,\n",
       "          2,   15,   16,    8,  216])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[24000,:]  #Here's what an example sequence looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras layers for (Vanilla) RNNs\n",
    "\n",
    "### Embedding Layer\n",
    "`keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n",
    "\n",
    "- This layer maps each integer into a distinct (dense) word vector of length `output_dim`.\n",
    "- Can think of this as learning a word vector embedding \"on the fly\" rather than using an existing mapping (like GloVe)\n",
    "- The `input_dim` should be the size of the vocabulary.\n",
    "- The `input_length` specifies the length of the sequences that the network expects.\n",
    "\n",
    "### SimpleRNN Layer\n",
    "`keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- This is the basic RNN, where the output is also fed back as the \"hidden state\" to the next iteration.\n",
    "- The parameter `units` gives the dimensionality of the output (and therefore the hidden state).  Note that typically there will be another layer after the RNN mapping the (RNN) output to the network output.  So we should think of this value as the desired dimensionality of the hidden state and not necessarily the desired output of the network.\n",
    "- Recall that there are two sets of weights, one for the \"recurrent\" phase and the other for the \"kernel\" phase.  These can be configured separately in terms of their initialization, regularization, etc.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's build a RNN\n",
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 75\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence and embeds it in a 50-dimensional vector\n",
    "model.add(Dropout(0.25))\n",
    "model.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(lr = .0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=rmsprop, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 75)          375000    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 75)          0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 5)                 405       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1536      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 377,198\n",
      "Trainable params: 377,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 148s 6ms/step - loss: 0.6513 - acc: 0.6576 - val_loss: 0.5825 - val_acc: 0.8012\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 143s 6ms/step - loss: 0.4999 - acc: 0.8241 - val_loss: 0.4789 - val_acc: 0.8167\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 143s 6ms/step - loss: 0.3592 - acc: 0.8644 - val_loss: 0.3589 - val_acc: 0.8582\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 143s 6ms/step - loss: 0.2949 - acc: 0.8851 - val_loss: 0.2955 - val_acc: 0.8819\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 146s 6ms/step - loss: 0.2607 - acc: 0.9004 - val_loss: 0.3324 - val_acc: 0.8637\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 145s 6ms/step - loss: 0.2388 - acc: 0.9069 - val_loss: 0.2711 - val_acc: 0.8893\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 146s 6ms/step - loss: 0.2251 - acc: 0.9127 - val_loss: 0.3231 - val_acc: 0.8655\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 146s 6ms/step - loss: 0.2138 - acc: 0.9183 - val_loss: 0.2824 - val_acc: 0.8856\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 146s 6ms/step - loss: 0.2069 - acc: 0.9216 - val_loss: 0.3038 - val_acc: 0.8758\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 146s 6ms/step - loss: 0.2007 - acc: 0.9233 - val_loss: 0.2766 - val_acc: 0.8877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252e5d45c50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Test score: 0.2766432267475128\n",
      "Test accuracy: 0.8876800000190734\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
