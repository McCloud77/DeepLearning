{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning 9 - Data Augmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/McCloud77/DeepLearning/blob/master/Deep_Learning_9_Data_Augmentation.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "G29Mt0NRE2w8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifying CIFAR-10 with Data Augmentation.\n",
        "\n",
        "Here we revisit CIFAR-10 and the networks we previously built.  We will use real-time data augmentation to try to improve our results."
      ]
    },
    {
      "metadata": {
        "id": "AChp1FD2E2xA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "912a33b3-a519-428e-e534-e3b1254b399a"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "K76Qbx9vE2xX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "930e3a61-a6e6-4383-ae13-ee0e2fd39e03"
      },
      "cell_type": "code",
      "source": [
        "# The data, shuffled and split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 10s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lNl3LfW7E2xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_b5cHB5sE2x1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YjkGvzTcE2x_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, let's use a smaller model with 181,162 prameters and train it with data augmentation."
      ]
    },
    {
      "metadata": {
        "id": "IQ6lbMSbE2yC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "outputId": "ed79bf85-c894-4aee-c68f-d424c45aa905"
      },
      "cell_type": "code",
      "source": [
        "# CNN using Keras' Sequential capabilities\n",
        "model_1 = Sequential()\n",
        "\n",
        "## 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_1.add(Conv2D(32, (5, 5),\n",
        "                   strides = (2,2),\n",
        "                   padding='same',\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "model_1.add(Activation('relu'))\n",
        "\n",
        "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_1.add(Conv2D(32, (5, 5),\n",
        "                   strides = (2,2)))\n",
        "model_1.add(Activation('relu'))\n",
        "\n",
        "## 2x2 max pooling reduces to 3 x 3 x 32\n",
        "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_1.add(Dropout(0.25))\n",
        "\n",
        "## Flatten turns 3x3x32 into 288x1\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(512))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(Dropout(0.5))\n",
        "model_1.add(Dense(num_classes))\n",
        "model_1.add(Activation('softmax'))\n",
        "\n",
        "model_1.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 32)        2432      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 6, 6, 32)          25632     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               147968    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 181,162\n",
            "Trainable params: 181,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_HKKbUTuE2yR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.rmsprop(lr=0.0005, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model_1.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8IpFya-E2yb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we define the `ImageDataGenerator` that we will use to serve images to our model during the training process.  Currently, it is configured to do some shifting and horizontal flipping."
      ]
    },
    {
      "metadata": {
        "id": "Bu37kHLOE2yg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "b52c72dd-4ba8-4173-9315-1de653d0b056"
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False)   # randomly flip images\n",
        "                   \n",
        "datagen.fit(x_train)      # This computes any statistics that may be needed (e.g. for centering) from the training set.\n",
        "    \n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model_1.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                   batch_size=batch_size),\n",
        "                      steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                      epochs=15,\n",
        "                      verbose=1,\n",
        "                      validation_data=(x_test, y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Epoch 1/15\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 1.7863 - acc: 0.3455 - val_loss: 1.5897 - val_acc: 0.4428\n",
            "Epoch 2/15\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.5317 - acc: 0.4459 - val_loss: 1.3375 - val_acc: 0.5224\n",
            "Epoch 3/15\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.4402 - acc: 0.4833 - val_loss: 1.2635 - val_acc: 0.5498\n",
            "Epoch 4/15\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 1.3868 - acc: 0.5017 - val_loss: 1.2090 - val_acc: 0.5698\n",
            "Epoch 5/15\n",
            "1562/1562 [==============================] - 40s 25ms/step - loss: 1.3475 - acc: 0.5172 - val_loss: 1.2300 - val_acc: 0.5656\n",
            "Epoch 6/15\n",
            "1562/1562 [==============================] - 40s 25ms/step - loss: 1.3306 - acc: 0.5257 - val_loss: 1.1787 - val_acc: 0.5823\n",
            "Epoch 7/15\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 1.3093 - acc: 0.5365 - val_loss: 1.1383 - val_acc: 0.5987\n",
            "Epoch 8/15\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.2959 - acc: 0.5440 - val_loss: 1.1443 - val_acc: 0.6020\n",
            "Epoch 9/15\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 1.2907 - acc: 0.5467 - val_loss: 1.1475 - val_acc: 0.5912\n",
            "Epoch 10/15\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.2832 - acc: 0.5498 - val_loss: 1.0952 - val_acc: 0.6180\n",
            "Epoch 11/15\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.2799 - acc: 0.5541 - val_loss: 1.0864 - val_acc: 0.6201\n",
            "Epoch 12/15\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.2776 - acc: 0.5544 - val_loss: 1.0937 - val_acc: 0.6218\n",
            "Epoch 13/15\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 1.2715 - acc: 0.5568 - val_loss: 1.1123 - val_acc: 0.6104\n",
            "Epoch 14/15\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.2800 - acc: 0.5568 - val_loss: 1.1102 - val_acc: 0.6085\n",
            "Epoch 15/15\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.2713 - acc: 0.5599 - val_loss: 1.1522 - val_acc: 0.6056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd05fe3dd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "5NdrgwEzE2yz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "6ccfd775-0992-47fb-a23a-7fcf81288907"
      },
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "scores = model_1.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 46us/step\n",
            "\n",
            "Test result: 60.560 loss: 1.152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RD7r-9X9E2zI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using Regularization and Different Learning Rates."
      ]
    },
    {
      "metadata": {
        "id": "vQ0oG4CwE2zM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mAt_Y_8vE2zV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Change learning rate based on epoch\n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    elif epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ocwYC8kEE2zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Calculate Z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4NQGZ4XnE2zq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weight_decay = 1e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YChJYU02E2z1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1130
        },
        "outputId": "ed4aa1f2-714a-460b-e2e5-edeb7a498cac"
      },
      "cell_type": "code",
      "source": [
        "model_2 = Sequential()\n",
        "\n",
        "model_2.add(Conv2D(32, (3,3), padding='same',\n",
        "                   kernel_regularizer=regularizers.l2(weight_decay),\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "model_2.add(BatchNormalization())\n",
        "model_2.add(Activation('relu'))\n",
        "      \n",
        "      \n",
        "model_2.add(Conv2D(32, (3,3), padding='same',\n",
        "                   kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_2.add(BatchNormalization())\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model_2.add(Dropout(0.2))\n",
        "\n",
        "model_2.add(Conv2D(64, (3,3), padding='same',\n",
        "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_2.add(BatchNormalization())\n",
        "model_2.add(Activation('relu'))\n",
        "            \n",
        "model_2.add(Conv2D(64, (3,3), padding='same',\n",
        "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_2.add(BatchNormalization())\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model_2.add(Dropout(0.3))\n",
        "            \n",
        "model_2.add(Conv2D(128, (3,3), padding='same',\n",
        "                   kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_2.add(BatchNormalization())\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(Conv2D(128, (3,3), padding='same',\n",
        "                   kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_2.add(BatchNormalization())\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model_2.add(Dropout(0.4))\n",
        "\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model_2.summary()            "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CP4bGAvYE2z_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(rotation_range=15,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             horizontal_flip=True,)\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XxDXjLkmE20O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4753
        },
        "outputId": "24db12a9-7d78-43bb-a505-561815c457de"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
        "\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt_rms, metrics=['accuracy'])\n",
        "\n",
        "model_2.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 1.6221 - acc: 0.4514 - val_loss: 1.5179 - val_acc: 0.5230\n",
            "Epoch 2/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 1.1759 - acc: 0.6079 - val_loss: 1.0990 - val_acc: 0.6460\n",
            "Epoch 3/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 1.0508 - acc: 0.6653 - val_loss: 0.9052 - val_acc: 0.7149\n",
            "Epoch 4/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.9853 - acc: 0.6932 - val_loss: 0.9961 - val_acc: 0.7000\n",
            "Epoch 5/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.9465 - acc: 0.7111 - val_loss: 0.9200 - val_acc: 0.7314\n",
            "Epoch 6/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.9102 - acc: 0.7255 - val_loss: 0.9043 - val_acc: 0.7441\n",
            "Epoch 7/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.8982 - acc: 0.7357 - val_loss: 0.8815 - val_acc: 0.7446\n",
            "Epoch 8/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8696 - acc: 0.7473 - val_loss: 0.7655 - val_acc: 0.7865\n",
            "Epoch 9/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8673 - acc: 0.7513 - val_loss: 0.8734 - val_acc: 0.7622\n",
            "Epoch 10/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.8533 - acc: 0.7571 - val_loss: 0.7802 - val_acc: 0.7871\n",
            "Epoch 11/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.8446 - acc: 0.7648 - val_loss: 0.7729 - val_acc: 0.7857\n",
            "Epoch 12/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8414 - acc: 0.7658 - val_loss: 0.9100 - val_acc: 0.7608\n",
            "Epoch 13/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.8413 - acc: 0.7672 - val_loss: 0.7651 - val_acc: 0.7969\n",
            "Epoch 14/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.8321 - acc: 0.7741 - val_loss: 0.7812 - val_acc: 0.7977\n",
            "Epoch 15/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.8442 - acc: 0.7755 - val_loss: 0.9143 - val_acc: 0.7647\n",
            "Epoch 16/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.8783 - acc: 0.7738 - val_loss: 0.9108 - val_acc: 0.7690\n",
            "Epoch 17/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.8375 - acc: 0.7791 - val_loss: 0.8445 - val_acc: 0.7752\n",
            "Epoch 18/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.8397 - acc: 0.7803 - val_loss: 0.8077 - val_acc: 0.7911\n",
            "Epoch 19/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.8471 - acc: 0.7780 - val_loss: 0.8155 - val_acc: 0.7905\n",
            "Epoch 20/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.8176 - acc: 0.7856 - val_loss: 0.7993 - val_acc: 0.7924\n",
            "Epoch 21/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.8233 - acc: 0.7854 - val_loss: 0.8241 - val_acc: 0.7858\n",
            "Epoch 22/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8106 - acc: 0.7883 - val_loss: 0.8310 - val_acc: 0.7939\n",
            "Epoch 23/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8256 - acc: 0.7879 - val_loss: 0.8109 - val_acc: 0.7972\n",
            "Epoch 24/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8161 - acc: 0.7893 - val_loss: 0.7100 - val_acc: 0.8219\n",
            "Epoch 25/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.8046 - acc: 0.7918 - val_loss: 0.7493 - val_acc: 0.8131\n",
            "Epoch 26/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.8868 - acc: 0.7876 - val_loss: 1.0032 - val_acc: 0.7641\n",
            "Epoch 27/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.8139 - acc: 0.7936 - val_loss: 0.8022 - val_acc: 0.8042\n",
            "Epoch 28/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.8004 - acc: 0.7966 - val_loss: 0.7619 - val_acc: 0.8083\n",
            "Epoch 29/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.8010 - acc: 0.7946 - val_loss: 0.7757 - val_acc: 0.8091\n",
            "Epoch 30/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8664 - acc: 0.7876 - val_loss: 0.8668 - val_acc: 0.7863\n",
            "Epoch 31/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8174 - acc: 0.7925 - val_loss: 0.8164 - val_acc: 0.8090\n",
            "Epoch 32/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.7963 - acc: 0.7978 - val_loss: 0.7829 - val_acc: 0.8077\n",
            "Epoch 33/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7965 - acc: 0.7992 - val_loss: 0.7263 - val_acc: 0.8205\n",
            "Epoch 34/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8679 - acc: 0.7889 - val_loss: 0.8543 - val_acc: 0.7958\n",
            "Epoch 35/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8029 - acc: 0.7970 - val_loss: 0.7450 - val_acc: 0.8277\n",
            "Epoch 36/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.7995 - acc: 0.8014 - val_loss: 0.8031 - val_acc: 0.7992\n",
            "Epoch 37/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.7867 - acc: 0.8023 - val_loss: 0.7725 - val_acc: 0.8064\n",
            "Epoch 38/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7871 - acc: 0.8019 - val_loss: 0.8242 - val_acc: 0.7948\n",
            "Epoch 39/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8751 - acc: 0.7932 - val_loss: 0.7979 - val_acc: 0.8092\n",
            "Epoch 40/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8203 - acc: 0.7995 - val_loss: 0.9038 - val_acc: 0.7750\n",
            "Epoch 41/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.7938 - acc: 0.8031 - val_loss: 0.9207 - val_acc: 0.7774\n",
            "Epoch 42/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7851 - acc: 0.8043 - val_loss: 0.7578 - val_acc: 0.8191\n",
            "Epoch 43/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 1.0312 - acc: 0.7868 - val_loss: 0.8361 - val_acc: 0.8226\n",
            "Epoch 44/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.9560 - acc: 0.7897 - val_loss: 0.7774 - val_acc: 0.8182\n",
            "Epoch 45/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.8152 - acc: 0.8020 - val_loss: 0.9621 - val_acc: 0.8096\n",
            "Epoch 46/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.8355 - acc: 0.7994 - val_loss: 0.7590 - val_acc: 0.8242\n",
            "Epoch 47/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.7899 - acc: 0.8058 - val_loss: 0.6659 - val_acc: 0.8431\n",
            "Epoch 48/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.7856 - acc: 0.8046 - val_loss: 0.7290 - val_acc: 0.8239\n",
            "Epoch 49/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 1.2043 - acc: 0.7809 - val_loss: 1.1143 - val_acc: 0.7929\n",
            "Epoch 50/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 1.0936 - acc: 0.7845 - val_loss: 0.9817 - val_acc: 0.8034\n",
            "Epoch 51/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.9618 - acc: 0.7931 - val_loss: 0.9945 - val_acc: 0.7934\n",
            "Epoch 52/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.9247 - acc: 0.7953 - val_loss: 0.9587 - val_acc: 0.7885\n",
            "Epoch 53/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.8340 - acc: 0.8016 - val_loss: 0.9490 - val_acc: 0.7771\n",
            "Epoch 54/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.9144 - acc: 0.7973 - val_loss: 0.8520 - val_acc: 0.8059\n",
            "Epoch 55/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8743 - acc: 0.7992 - val_loss: 0.7937 - val_acc: 0.8164\n",
            "Epoch 56/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.8051 - acc: 0.8054 - val_loss: 0.7270 - val_acc: 0.8315\n",
            "Epoch 57/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7820 - acc: 0.8074 - val_loss: 0.7219 - val_acc: 0.8341\n",
            "Epoch 58/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7846 - acc: 0.8068 - val_loss: 0.7400 - val_acc: 0.8251\n",
            "Epoch 59/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7860 - acc: 0.8104 - val_loss: 0.7103 - val_acc: 0.8314\n",
            "Epoch 60/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7711 - acc: 0.8119 - val_loss: 0.7275 - val_acc: 0.8267\n",
            "Epoch 61/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.7620 - acc: 0.8104 - val_loss: 0.7544 - val_acc: 0.8197\n",
            "Epoch 62/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.7728 - acc: 0.8106 - val_loss: 0.7184 - val_acc: 0.8269\n",
            "Epoch 63/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7655 - acc: 0.8111 - val_loss: 0.7488 - val_acc: 0.8173\n",
            "Epoch 64/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7643 - acc: 0.8105 - val_loss: 0.7949 - val_acc: 0.8125\n",
            "Epoch 65/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7707 - acc: 0.8082 - val_loss: 0.7054 - val_acc: 0.8266\n",
            "Epoch 66/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.7574 - acc: 0.8105 - val_loss: 0.7426 - val_acc: 0.8214\n",
            "Epoch 67/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.7933 - acc: 0.8084 - val_loss: 0.9557 - val_acc: 0.7800\n",
            "Epoch 68/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.8187 - acc: 0.8055 - val_loss: 0.7316 - val_acc: 0.8252\n",
            "Epoch 69/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.7647 - acc: 0.8093 - val_loss: 0.6982 - val_acc: 0.8322\n",
            "Epoch 70/125\n",
            "1562/1562 [==============================] - 66s 43ms/step - loss: 0.7734 - acc: 0.8100 - val_loss: 0.7399 - val_acc: 0.8304\n",
            "Epoch 71/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 1.0607 - acc: 0.7921 - val_loss: 0.9077 - val_acc: 0.8109\n",
            "Epoch 72/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.9630 - acc: 0.7980 - val_loss: 0.7804 - val_acc: 0.8242\n",
            "Epoch 73/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.8067 - acc: 0.8069 - val_loss: 0.8641 - val_acc: 0.7894\n",
            "Epoch 74/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.7728 - acc: 0.8110 - val_loss: 0.7630 - val_acc: 0.8270\n",
            "Epoch 75/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.7608 - acc: 0.8137 - val_loss: 0.9181 - val_acc: 0.7877\n",
            "Epoch 76/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.7638 - acc: 0.8118 - val_loss: 0.6911 - val_acc: 0.8356\n",
            "Epoch 77/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.7043 - acc: 0.8289 - val_loss: 0.6769 - val_acc: 0.8440\n",
            "Epoch 78/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6772 - acc: 0.8336 - val_loss: 0.6482 - val_acc: 0.8459\n",
            "Epoch 79/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6652 - acc: 0.8378 - val_loss: 0.6341 - val_acc: 0.8504\n",
            "Epoch 80/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6661 - acc: 0.8382 - val_loss: 0.6330 - val_acc: 0.8509\n",
            "Epoch 81/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6484 - acc: 0.8400 - val_loss: 0.6064 - val_acc: 0.8581\n",
            "Epoch 82/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6514 - acc: 0.8387 - val_loss: 0.6544 - val_acc: 0.8479\n",
            "Epoch 83/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.6409 - acc: 0.8411 - val_loss: 0.6243 - val_acc: 0.8501\n",
            "Epoch 84/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6440 - acc: 0.8410 - val_loss: 0.6019 - val_acc: 0.8542\n",
            "Epoch 85/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6410 - acc: 0.8394 - val_loss: 0.7033 - val_acc: 0.8343\n",
            "Epoch 86/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.6362 - acc: 0.8417 - val_loss: 0.6100 - val_acc: 0.8525\n",
            "Epoch 87/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6310 - acc: 0.8411 - val_loss: 0.6480 - val_acc: 0.8403\n",
            "Epoch 88/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6295 - acc: 0.8411 - val_loss: 0.5694 - val_acc: 0.8648\n",
            "Epoch 89/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6283 - acc: 0.8427 - val_loss: 0.5897 - val_acc: 0.8614\n",
            "Epoch 90/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6293 - acc: 0.8433 - val_loss: 0.6439 - val_acc: 0.8419\n",
            "Epoch 91/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6300 - acc: 0.8403 - val_loss: 0.6786 - val_acc: 0.8341\n",
            "Epoch 92/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.6293 - acc: 0.8413 - val_loss: 0.5785 - val_acc: 0.8560\n",
            "Epoch 93/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6178 - acc: 0.8461 - val_loss: 0.6177 - val_acc: 0.8477\n",
            "Epoch 94/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6251 - acc: 0.8430 - val_loss: 0.6873 - val_acc: 0.8351\n",
            "Epoch 95/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6221 - acc: 0.8421 - val_loss: 0.6268 - val_acc: 0.8453\n",
            "Epoch 96/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.6253 - acc: 0.8435 - val_loss: 0.5663 - val_acc: 0.8635\n",
            "Epoch 97/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.6215 - acc: 0.8418 - val_loss: 0.5809 - val_acc: 0.8557\n",
            "Epoch 98/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.6169 - acc: 0.8415 - val_loss: 0.5973 - val_acc: 0.8548\n",
            "Epoch 99/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6234 - acc: 0.8412 - val_loss: 0.5546 - val_acc: 0.8643\n",
            "Epoch 100/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.6140 - acc: 0.8427 - val_loss: 0.5549 - val_acc: 0.8644\n",
            "Epoch 101/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.6109 - acc: 0.8438 - val_loss: 0.6273 - val_acc: 0.8498\n",
            "Epoch 102/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.6152 - acc: 0.8432 - val_loss: 0.5998 - val_acc: 0.8581\n",
            "Epoch 103/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.6202 - acc: 0.8428 - val_loss: 0.7476 - val_acc: 0.8174\n",
            "Epoch 104/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6171 - acc: 0.8423 - val_loss: 0.5816 - val_acc: 0.8572\n",
            "Epoch 105/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6101 - acc: 0.8454 - val_loss: 0.5792 - val_acc: 0.8626\n",
            "Epoch 106/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6131 - acc: 0.8447 - val_loss: 0.6565 - val_acc: 0.8329\n",
            "Epoch 107/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6146 - acc: 0.8425 - val_loss: 0.5753 - val_acc: 0.8600\n",
            "Epoch 108/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6095 - acc: 0.8444 - val_loss: 0.7116 - val_acc: 0.8254\n",
            "Epoch 109/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6074 - acc: 0.8437 - val_loss: 0.5671 - val_acc: 0.8564\n",
            "Epoch 110/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6162 - acc: 0.8426 - val_loss: 0.5714 - val_acc: 0.8603\n",
            "Epoch 111/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6161 - acc: 0.8430 - val_loss: 0.6669 - val_acc: 0.8284\n",
            "Epoch 112/125\n",
            "1562/1562 [==============================] - 66s 42ms/step - loss: 0.6102 - acc: 0.8435 - val_loss: 0.6351 - val_acc: 0.8431\n",
            "Epoch 113/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6082 - acc: 0.8450 - val_loss: 0.6238 - val_acc: 0.8446\n",
            "Epoch 114/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6056 - acc: 0.8448 - val_loss: 0.6345 - val_acc: 0.8409\n",
            "Epoch 115/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6050 - acc: 0.8459 - val_loss: 0.6664 - val_acc: 0.8325\n",
            "Epoch 116/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.5998 - acc: 0.8459 - val_loss: 0.5568 - val_acc: 0.8616\n",
            "Epoch 117/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.6140 - acc: 0.8455 - val_loss: 0.6034 - val_acc: 0.8493\n",
            "Epoch 118/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.6152 - acc: 0.8440 - val_loss: 0.5674 - val_acc: 0.8592\n",
            "Epoch 119/125\n",
            "1562/1562 [==============================] - 68s 44ms/step - loss: 0.6011 - acc: 0.8451 - val_loss: 0.5396 - val_acc: 0.8708\n",
            "Epoch 120/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.6110 - acc: 0.8436 - val_loss: 0.5979 - val_acc: 0.8608\n",
            "Epoch 121/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6054 - acc: 0.8440 - val_loss: 0.5865 - val_acc: 0.8590\n",
            "Epoch 122/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6079 - acc: 0.8455 - val_loss: 0.5515 - val_acc: 0.8621\n",
            "Epoch 123/125\n",
            "1562/1562 [==============================] - 67s 43ms/step - loss: 0.6150 - acc: 0.8432 - val_loss: 0.6183 - val_acc: 0.8482\n",
            "Epoch 124/125\n",
            "1562/1562 [==============================] - 69s 44ms/step - loss: 0.5995 - acc: 0.8447 - val_loss: 0.6151 - val_acc: 0.8436\n",
            "Epoch 125/125\n",
            "1562/1562 [==============================] - 68s 43ms/step - loss: 0.6022 - acc: 0.8447 - val_loss: 0.6679 - val_acc: 0.8372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd058a51b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "JN4KBR0JE20j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "691adb47-4df0-4efd-f0bf-47271a739d5c"
      },
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "scores = model_2.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 195us/step\n",
            "\n",
            "Test result: 83.720 loss: 0.668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UGfj1ENcE20z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4aDA4PxvE209",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbMucw1GE21K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GaH0YWoiE21U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gORq53PE21g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}